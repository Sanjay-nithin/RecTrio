{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcaae8b",
   "metadata": {},
   "source": [
    "# Custom CNN + Text Encoder Training for Image-Text Retrieval (Fashion MNIST)\n",
    "\n",
    "This notebook trains a dual-encoder model on Fashion MNIST dataset:\n",
    "- **Image Encoder**: Custom CNN (handles grayscale images)\n",
    "- **Text Encoder**: LSTM-based text encoder\n",
    "- **Dataset**: Fashion MNIST (70,000 images, 10 fashion categories)\n",
    "- **Objective**: Project images and text to shared embedding space\n",
    "- **Optimization**: Intel CPU optimized with OpenVINO\n",
    "\n",
    "## Features:\n",
    "- Contrastive learning (CLIP-style)\n",
    "- Supports both image and text queries\n",
    "- Intel CPU optimized\n",
    "- FAISS-based similarity search\n",
    "- Grayscale image support (Fashion MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9615bfa0",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3208d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "! pip install torch torchvision\n",
    "! pip install openvino openvino-dev\n",
    "! pip install faiss-cpu\n",
    "! pip install pillow numpy matplotlib seaborn pandas scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0a8046",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Clear any cached text_descriptions module\n",
    "if 'text_descriptions' in sys.modules:\n",
    "    del sys.modules['text_descriptions']\n",
    "\n",
    "# Add Fashion MNIST path at the beginning of sys.path\n",
    "fashion_mnist_path = r'e:\\Projects\\AI Based\\RecTrio\\datasets\\fashion_mnist'\n",
    "if fashion_mnist_path in sys.path:\n",
    "    sys.path.remove(fashion_mnist_path)\n",
    "sys.path.insert(0, fashion_mnist_path)\n",
    "\n",
    "# Now import from the correct location\n",
    "from text_descriptions import FASHION_DESCRIPTIONS, CLASSES\n",
    "\n",
    "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "print(f\"✓ Available classes: {CLASSES}\")\n",
    "print(f\"✓ Number of classes: {len(CLASSES)}\")\n",
    "print(f\"✓ Sample class descriptions: {len(FASHION_DESCRIPTIONS[CLASSES[0]])} per class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00f82ac",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a01ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATASET_PATH = Path(r'e:\\Projects\\AI Based\\RecTrio\\datasets\\fashion_mnist\\processed\\train')\n",
    "OUTPUT_DIR = Path(r'e:\\Projects\\AI Based\\RecTrio\\V1\\models\\fashion_cnn')\n",
    "VECTOR_DB_DIR = OUTPUT_DIR / 'vector_db'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model Configuration\n",
    "IMAGE_SIZE = 224\n",
    "INPUT_CHANNELS = 1  # Grayscale images (Fashion MNIST)\n",
    "EMBEDDING_DIM = 256  # Shared embedding space dimension\n",
    "CNN_FEATURES = 512   # CNN output features\n",
    "TEXT_HIDDEN_DIM = 256  # LSTM hidden dimension\n",
    "MAX_TEXT_LENGTH = 20  # Maximum words in description\n",
    "\n",
    "# Training Configuration\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "TEMPERATURE = 0.07  # Contrastive loss temperature\n",
    "\n",
    "# Data split\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "# Random seed\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Device\n",
    "DEVICE = 'cpu'  # Intel CPU optimized\n",
    "print(f\"✓ Dataset: {DATASET_PATH}\")\n",
    "print(f\"✓ Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"✓ Vector DB directory: {VECTOR_DB_DIR}\")\n",
    "print(f\"✓ Device: {DEVICE}\")\n",
    "print(f\"✓ Input channels: {INPUT_CHANNELS} (grayscale)\")\n",
    "print(f\"✓ Embedding dimension: {EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27bd3c2",
   "metadata": {},
   "source": [
    "## 4. Set Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d895e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "print(f\"✓ Random seed set to {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c78971",
   "metadata": {},
   "source": [
    "## 5. Build Vocabulary from Text Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Simple vocabulary for text encoding\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.word_count = {}\n",
    "        self.n_words = 2\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.lower().split():\n",
    "            self.add_word(word)\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.n_words\n",
    "            self.idx2word[self.n_words] = word\n",
    "            self.word_count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word_count[word] += 1\n",
    "    \n",
    "    def encode(self, sentence, max_length=20):\n",
    "        \"\"\"Encode sentence to indices\"\"\"\n",
    "        words = sentence.lower().split()[:max_length]\n",
    "        indices = [self.word2idx.get(word, 1) for word in words]\n",
    "        # Pad to max_length\n",
    "        indices += [0] * (max_length - len(indices))\n",
    "        return indices\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = Vocabulary()\n",
    "for class_name, descriptions in FASHION_DESCRIPTIONS.items():\n",
    "    for desc in descriptions:\n",
    "        vocab.add_sentence(desc)\n",
    "\n",
    "print(f\"✓ Vocabulary size: {vocab.n_words}\")\n",
    "print(f\"  Sample words: {list(vocab.word2idx.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df232d5",
   "metadata": {},
   "source": [
    "## 6. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextDataset(Dataset):\n",
    "    \"\"\"Dataset that pairs Fashion MNIST images with text descriptions\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, vocab, transform=None, split='train'):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        \n",
    "        # Collect all images\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(CLASSES)}\n",
    "        \n",
    "        valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "        \n",
    "        for class_name in CLASSES:\n",
    "            class_dir = self.root_dir / class_name\n",
    "            if not class_dir.exists():\n",
    "                print(f\"Warning: {class_dir} not found, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            for img_path in class_dir.iterdir():\n",
    "                if img_path.suffix.lower() in valid_extensions:\n",
    "                    self.image_paths.append(str(img_path))\n",
    "                    self.labels.append(class_name)\n",
    "        \n",
    "        print(f\"✓ {split} dataset: {len(self.image_paths)} images across {len(CLASSES)} classes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image (Fashion MNIST is grayscale)\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "        \n",
    "        # Convert grayscale to 3-channel for compatibility\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get class and random description\n",
    "        class_name = self.labels[idx]\n",
    "        descriptions = FASHION_DESCRIPTIONS[class_name]\n",
    "        description = random.choice(descriptions)\n",
    "        \n",
    "        # Encode text\n",
    "        text_indices = self.vocab.encode(description, MAX_TEXT_LENGTH)\n",
    "        text_tensor = torch.LongTensor(text_indices)\n",
    "        \n",
    "        # Class index\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "        \n",
    "        return image, text_tensor, class_idx\n",
    "\n",
    "print(\"✓ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88552b0b",
   "metadata": {},
   "source": [
    "## 7. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b185fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Custom CNN for image encoding\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Block 1: 224x224 -> 112x112\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Block 2: 112x112 -> 56x56\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Block 3: 56x56 -> 28x28\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Block 4: 28x28 -> 14x14\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Projection to embedding space\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.projection(x)\n",
    "        # L2 normalize\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"LSTM-based text encoder\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, 128, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(128, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Projection to embedding space\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, max_length]\n",
    "        embedded = self.embedding(x)  # [batch, max_length, 128]\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # Use final hidden state from both directions\n",
    "        hidden = torch.cat([hidden[0], hidden[1]], dim=1)  # [batch, hidden_dim*2]\n",
    "        \n",
    "        output = self.projection(hidden)\n",
    "        # L2 normalize\n",
    "        output = F.normalize(output, p=2, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class DualEncoder(nn.Module):\n",
    "    \"\"\"Combined image and text encoder\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=256, text_hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder(embedding_dim)\n",
    "        self.text_encoder = TextEncoder(vocab_size, embedding_dim, text_hidden_dim)\n",
    "    \n",
    "    def forward(self, images, texts):\n",
    "        image_embeddings = self.image_encoder(images)\n",
    "        text_embeddings = self.text_encoder(texts)\n",
    "        return image_embeddings, text_embeddings\n",
    "\n",
    "print(\"✓ Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f9f5e0",
   "metadata": {},
   "source": [
    "## 8. Contrastive Loss (CLIP-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91da9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"InfoNCE / CLIP-style contrastive loss\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, image_embeddings, text_embeddings):\n",
    "        # Normalize embeddings\n",
    "        image_embeddings = F.normalize(image_embeddings, p=2, dim=1)\n",
    "        text_embeddings = F.normalize(text_embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        logits = torch.matmul(image_embeddings, text_embeddings.t()) / self.temperature\n",
    "        \n",
    "        batch_size = image_embeddings.shape[0]\n",
    "        labels = torch.arange(batch_size, device=image_embeddings.device)\n",
    "        \n",
    "        # Symmetric loss (image-to-text + text-to-image)\n",
    "        loss_i2t = F.cross_entropy(logits, labels)\n",
    "        loss_t2i = F.cross_entropy(logits.t(), labels)\n",
    "        \n",
    "        loss = (loss_i2t + loss_t2i) / 2\n",
    "        \n",
    "        # Compute accuracy\n",
    "        with torch.no_grad():\n",
    "            preds_i2t = torch.argmax(logits, dim=1)\n",
    "            preds_t2i = torch.argmax(logits.t(), dim=1)\n",
    "            acc_i2t = (preds_i2t == labels).float().mean()\n",
    "            acc_t2i = (preds_t2i == labels).float().mean()\n",
    "            accuracy = (acc_i2t + acc_t2i) / 2\n",
    "        \n",
    "        return loss, accuracy\n",
    "\n",
    "print(\"✓ Contrastive loss defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fbdeaf",
   "metadata": {},
   "source": [
    "## 9. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9755a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "full_dataset = ImageTextDataset(DATASET_PATH, vocab, transform=train_transform, split='full')\n",
    "\n",
    "# Split into train and val\n",
    "dataset_size = len(full_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_size = int(TRAIN_SPLIT * dataset_size)\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "# Update val_dataset transform\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cd58eb",
   "metadata": {},
   "source": [
    "## 10. Initialize Model and Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = DualEncoder(\n",
    "    vocab_size=vocab.n_words,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    text_hidden_dim=TEXT_HIDDEN_DIM\n",
    ").to(DEVICE)\n",
    "\n",
    "# Loss function\n",
    "criterion = ContrastiveLoss(temperature=TEMPERATURE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model initialized on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf799c2",
   "metadata": {},
   "source": [
    "## 11. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4efe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for images, texts, _ in pbar:\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        image_emb, text_emb = model(images, texts)\n",
    "        loss, accuracy = criterion(image_emb, text_emb)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy.item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{accuracy.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(dataloader), total_accuracy / len(dataloader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Validation')\n",
    "    for images, texts, _ in pbar:\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        image_emb, text_emb = model(images, texts)\n",
    "        loss, accuracy = criterion(image_emb, text_emb)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy.item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{accuracy.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(dataloader), total_accuracy / len(dataloader)\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc81f5",
   "metadata": {},
   "source": [
    "## 12. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a368bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model_path = OUTPUT_DIR / 'best_multimodal_model.pth'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "    print(f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'vocab': vocab,\n",
    "            'config': {\n",
    "                'embedding_dim': EMBEDDING_DIM,\n",
    "                'text_hidden_dim': TEXT_HIDDEN_DIM,\n",
    "                'max_text_length': MAX_TEXT_LENGTH,\n",
    "                'image_size': IMAGE_SIZE\n",
    "            }\n",
    "        }, best_model_path)\n",
    "        print(f\"✓ Best model saved (val_acc: {val_acc:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Completed!\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Model saved to: {best_model_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9be93b",
   "metadata": {},
   "source": [
    "## 13. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d3966",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Plot loss\n",
    "axes[0].plot(epochs_range, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "axes[0].plot(epochs_range, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "axes[1].plot(epochs_range, history['train_acc'], 'b-', label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(epochs_range, history['val_acc'], 'r-', label='Val Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Plot saved to {OUTPUT_DIR / 'training_history.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a7154",
   "metadata": {},
   "source": [
    "## 14. Save Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = OUTPUT_DIR / 'vocabulary.pkl'\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "print(f\"✓ Vocabulary saved to {vocab_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c761d8f8",
   "metadata": {},
   "source": [
    "## 15. Convert to OpenVINO (Intel CPU Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3d97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Extract separate encoders\n",
    "image_encoder = model.image_encoder\n",
    "text_encoder = model.text_encoder\n",
    "\n",
    "# Convert image encoder\n",
    "print(\"Converting image encoder to OpenVINO...\")\n",
    "dummy_image = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "image_ov_model = ov.convert_model(image_encoder, example_input=dummy_image)\n",
    "ov.save_model(image_ov_model, OUTPUT_DIR / 'image_encoder.xml')\n",
    "print(f\"✓ Image encoder saved to {OUTPUT_DIR / 'image_encoder.xml'}\")\n",
    "\n",
    "# Convert text encoder\n",
    "print(\"Converting text encoder to OpenVINO...\")\n",
    "dummy_text = torch.randint(0, vocab.n_words, (1, MAX_TEXT_LENGTH))\n",
    "text_ov_model = ov.convert_model(text_encoder, example_input=dummy_text)\n",
    "ov.save_model(text_ov_model, OUTPUT_DIR / 'text_encoder.xml')\n",
    "print(f\"✓ Text encoder saved to {OUTPUT_DIR / 'text_encoder.xml'}\")\n",
    "\n",
    "print(\"\\n✓ Models converted to OpenVINO IR format for Intel CPU optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5f51b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Files Created:\n",
    "- `best_multimodal_model.pth` - PyTorch model checkpoint\n",
    "- `vocabulary.pkl` - Text vocabulary\n",
    "- `image_encoder.xml/.bin` - OpenVINO image encoder (Intel optimized)\n",
    "- `text_encoder.xml/.bin` - OpenVINO text encoder (Intel optimized)\n",
    "- `training_history.png` - Training curves\n",
    "\n",
    "### Next Steps:\n",
    "1. Build embeddings database for all images\n",
    "2. Create inference notebook for image/text queries\n",
    "3. Deploy using OpenVINO for fast CPU inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
