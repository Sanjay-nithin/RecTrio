{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c24c8a3",
   "metadata": {},
   "source": [
    "# Build CLIP Embeddings with OpenVINO\n",
    "\n",
    "This notebook converts CLIP model to OpenVINO format and builds embeddings database for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a33665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\sanjay nithin\\appdata\\local\\temp\\pip-req-build-bqogca4c\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from clip==1.0) (25.0)\n",
      "Requirement already satisfied: regex in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from clip==1.0) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from clip==1.0) (2.9.1)\n",
      "Requirement already satisfied: torchvision in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from clip==1.0) (0.24.1)\n",
      "Requirement already satisfied: wcwidth in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from ftfy->clip==1.0) (0.2.14)\n",
      "Requirement already satisfied: filelock in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (2025.12.0)\n",
      "Requirement already satisfied: numpy in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torchvision->clip==1.0) (12.0.0)\n",
      "Requirement already satisfied: colorama in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from jinja2->torch->clip==1.0) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\Sanjay Nithin\\AppData\\Local\\Temp\\pip-req-build-bqogca4c'\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openvino in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (2024.6.0)\n",
      "Requirement already satisfied: faiss-cpu in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (1.13.2)\n",
      "Requirement already satisfied: pillow in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (12.0.0)\n",
      "Requirement already satisfied: tqdm in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.16.6 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from openvino) (1.26.4)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.2.1 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from openvino) (2025.2.0)\n",
      "Requirement already satisfied: packaging in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from openvino) (25.0)\n",
      "Requirement already satisfied: colorama in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://github.com/openai/CLIP.git\n",
    "! pip install openvino faiss-cpu pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3ffe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "from openvino.runtime import Core\n",
    "import faiss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b0744",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe4f189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model directory: e:\\Projects\\AI Based\\RecTrio\\V2\\models\n",
      "✓ Vector DB directory: e:\\Projects\\AI Based\\RecTrio\\V2\\vector_db\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = Path(r\"e:\\Projects\\AI Based\\RecTrio\\datasets\\animals\\raw-img\")\n",
    "MODEL_DIR = Path(r\"e:\\Projects\\AI Based\\RecTrio\\V2\\models\")\n",
    "VECTOR_DB_DIR = Path(r\"e:\\Projects\\AI Based\\RecTrio\\V2\\vector_db\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CLIP_MODEL_NAME = \"ViT-B/32\"  # Fast GitHub CLIP model\n",
    "VISION_MODEL_PATH = MODEL_DIR / \"clip_vision_model.xml\"\n",
    "TEXT_MODEL_PATH = MODEL_DIR / \"clip_text_model.xml\"\n",
    "\n",
    "# Embedding database files\n",
    "EMBEDDINGS_FILE = VECTOR_DB_DIR / \"embeddings.npy\"\n",
    "METADATA_FILE = VECTOR_DB_DIR / \"metadata.pkl\"\n",
    "FAISS_INDEX_FILE = VECTOR_DB_DIR / \"faiss_index.bin\"\n",
    "\n",
    "print(f\"✓ Model directory: {MODEL_DIR}\")\n",
    "print(f\"✓ Vector DB directory: {VECTOR_DB_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6afa48",
   "metadata": {},
   "source": [
    "## Load and Convert CLIP Model to OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f1ae19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model from GitHub (fast loading)...\n",
      "✓ CLIP ViT-B/32 loaded successfully\n",
      "Image input size: 224x224\n",
      "Embedding dimension: 512\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading CLIP model from GitHub (fast loading)...\")\n",
    "model, preprocess = clip.load(CLIP_MODEL_NAME, device=\"cpu\")\n",
    "model.eval()\n",
    "print(f\"✓ CLIP {CLIP_MODEL_NAME} loaded successfully\")\n",
    "print(f\"Image input size: 224x224\")\n",
    "print(f\"Embedding dimension: 512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26baccd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting vision model to OpenVINO...\n"
     ]
    },
    {
     "ename": "OpConversionFailure",
     "evalue": "Check 'is_conversion_successful' failed at src\\frontends\\pytorch\\src\\frontend.cpp:171:\nFrontEnd API failed with OpConversionFailure:\nModel wasn't fully converted. Failed operations detailed log:\n-- prim::GetAttr with a message:\nException happened during conversion of operation prim::GetAttr with schema (no schema)\nKeyboardInterrupt: <EMPTY MESSAGE>\n\nAt:\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\frontend\\pytorch\\utils.py(96): get_value_from_getattr\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\frontend\\pytorch\\ts_decoder.py(380): try_decode_get_attr\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\frontend\\pytorch\\ts_decoder.py(288): visit_subgraph\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\frontend\\frontend.py(18): convert\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\tools\\ovc\\moc_frontend\\pipeline.py(143): moc_pipeline\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\tools\\ovc\\convert_impl.py(195): prepare_ir\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\tools\\ovc\\convert_impl.py(251): driver\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\tools\\ovc\\convert_impl.py(503): _convert\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\tools\\ovc\\convert.py(101): convert_model\n  C:\\Users\\Sanjay Nithin\\AppData\\Local\\Temp\\ipykernel_10780\\101921495.py(8): <module>\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3701): run_code\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3641): run_ast_nodes\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3400): run_cell_async\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py(128): _pseudo_sync_runner\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3178): _run_cell\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3123): run_cell\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\ipykernel\\zmqshell.py(663): run_cell\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py(458): do_execute\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py(827): execute_request\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py(366): execute_request\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py(471): dispatch_shell\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py(614): shell_main\n  C:\\Users\\Sanjay Nithin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py(80): _run\n  C:\\Users\\Sanjay Nithin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py(1922): _run_once\n  C:\\Users\\Sanjay Nithin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py(607): run_forever\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py(211): start\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\ipykernel\\kernelapp.py(758): start\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\traitlets\\config\\application.py(1075): launch_instance\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\ipykernel_launcher.py(18): <module>\n  <frozen runpy>(88): _run_code\n  <frozen runpy>(198): _run_module_as_main\n\nSummary:\n-- Conversion is failed for: prim::GetAttr\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpConversionFailure\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m dummy_input = torch.randn(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenvino\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mov\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m vision_ov_model = \u001b[43mov\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexample_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdummy_input\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m ov.save_model(vision_ov_model, VISION_MODEL_PATH)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Vision model saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVISION_MODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\tools\\ovc\\convert.py:101\u001b[39m, in \u001b[36mconvert_model\u001b[39m\u001b[34m(input_model, input, output, example_input, extension, verbose, share_weights)\u001b[39m\n\u001b[32m     99\u001b[39m logger_state = get_logger_state()\n\u001b[32m    100\u001b[39m cli_parser = get_all_cli_parser()\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m ov_model, _ = \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcli_parser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m restore_logger_state(logger_state)\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ov_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\tools\\ovc\\convert_impl.py:563\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(cli_parser, args, python_api_used)\u001b[39m\n\u001b[32m    561\u001b[39m send_conversion_result(\u001b[33m'\u001b[39m\u001b[33mfail\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m python_api_used:\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    565\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, argv\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\tools\\ovc\\convert_impl.py:503\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(cli_parser, args, python_api_used)\u001b[39m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m argv.framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_pytorch_decoder_for_model_on_disk(argv, args):\n\u001b[32m    498\u001b[39m     \u001b[38;5;66;03m# try to load a model from disk as TorchScript or ExportedProgram\u001b[39;00m\n\u001b[32m    499\u001b[39m     \u001b[38;5;66;03m# TorchScriptPythonDecoder or TorchFXPythonDecoder object will be assigned to argv.input_model\u001b[39;00m\n\u001b[32m    500\u001b[39m     \u001b[38;5;66;03m# saved TorchScript and ExportedModel model can be passed to both ovc tool and Python convert_model\u001b[39;00m\n\u001b[32m    501\u001b[39m     pytorch_model_on_disk = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m ov_model = \u001b[43mdriver\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconversion_parameters\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_default_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pytorch_model_on_disk:\n\u001b[32m    506\u001b[39m     \u001b[38;5;66;03m# release memory allocated for temporal object\u001b[39;00m\n\u001b[32m    507\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m argv.input_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\tools\\ovc\\convert_impl.py:251\u001b[39m, in \u001b[36mdriver\u001b[39m\u001b[34m(argv, non_default_params)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# Log dictionary with non-default cli parameters where complex classes are excluded.\u001b[39;00m\n\u001b[32m    249\u001b[39m log.debug(\u001b[38;5;28mstr\u001b[39m(non_default_params))\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m ov_model = moc_emit_ir(\u001b[43mprepare_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m, argv)\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ov_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\tools\\ovc\\convert_impl.py:195\u001b[39m, in \u001b[36mprepare_ir\u001b[39m\u001b[34m(argv)\u001b[39m\n\u001b[32m    193\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m extension \u001b[38;5;129;01min\u001b[39;00m filtered_extensions(argv.extension):\n\u001b[32m    194\u001b[39m             moc_front_end.add_extension(extension)\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     ov_model = \u001b[43mmoc_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmoc_front_end\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ov_model\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m argv.input_model:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\tools\\ovc\\moc_frontend\\pipeline.py:143\u001b[39m, in \u001b[36mmoc_pipeline\u001b[39m\u001b[34m(argv, moc_front_end)\u001b[39m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m to_override_all_outputs:\n\u001b[32m    141\u001b[39m         input_model.override_all_outputs(oplaces)\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     ov_model = \u001b[43mmoc_front_end\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ov_model\n\u001b[32m    146\u001b[39m argv.placeholder_shapes, argv.placeholder_data_types = convert_params_lists_to_dicts(\n\u001b[32m    147\u001b[39m     input_model, argv.placeholder_shapes, argv.placeholder_data_types)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\frontend\\frontend.py:18\u001b[39m, in \u001b[36mFrontEnd.convert\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: Union[Model, InputModel]) -> Model:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     converted_model = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, InputModel):\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Model(converted_model)\n",
      "\u001b[31mOpConversionFailure\u001b[39m: Check 'is_conversion_successful' failed at src\\frontends\\pytorch\\src\\frontend.cpp:171:\nFrontEnd API failed with OpConversionFailure:\nModel wasn't fully converted. Failed operations detailed log:\n-- prim::GetAttr with a message:\nException happened during conversion of operation prim::GetAttr with schema (no schema)\nKeyboardInterrupt: <EMPTY MESSAGE>\n\nAt:\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\frontend\\pytorch\\utils.py(96): get_value_from_getattr\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\frontend\\pytorch\\ts_decoder.py(380): try_decode_get_attr\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\frontend\\pytorch\\ts_decoder.py(288): visit_subgraph\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\frontend\\frontend.py(18): convert\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\tools\\ovc\\moc_frontend\\pipeline.py(143): moc_pipeline\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\tools\\ovc\\convert_impl.py(195): prepare_ir\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\tools\\ovc\\convert_impl.py(251): driver\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\tools\\ovc\\convert_impl.py(503): _convert\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\openvino\\tools\\ovc\\convert.py(101): convert_model\n  C:\\Users\\Sanjay Nithin\\AppData\\Local\\Temp\\ipykernel_10780\\101921495.py(8): <module>\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3701): run_code\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3641): run_ast_nodes\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3400): run_cell_async\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py(128): _pseudo_sync_runner\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3178): _run_cell\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3123): run_cell\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\ipykernel\\zmqshell.py(663): run_cell\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py(458): do_execute\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py(827): execute_request\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py(366): execute_request\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py(471): dispatch_shell\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py(614): shell_main\n  C:\\Users\\Sanjay Nithin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py(80): _run\n  C:\\Users\\Sanjay Nithin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py(1922): _run_once\n  C:\\Users\\Sanjay Nithin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py(607): run_forever\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py(211): start\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\ipykernel\\kernelapp.py(758): start\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\traitlets\\config\\application.py(1075): launch_instance\n  e:\\Projects\\AI Based\\RecTrio\\venv\\Lib\\site-packages\\ipykernel_launcher.py(18): <module>\n  <frozen runpy>(88): _run_code\n  <frozen runpy>(198): _run_module_as_main\n\nSummary:\n-- Conversion is failed for: prim::GetAttr\n"
     ]
    }
   ],
   "source": [
    "if not VISION_MODEL_PATH.exists():\n",
    "    print(\"Converting vision model to OpenVINO...\")\n",
    "    \n",
    "    # Create dummy input for vision model\n",
    "    dummy_input = torch.randn(1, 3, 224, 224)\n",
    "    \n",
    "    import openvino as ov\n",
    "    vision_ov_model = ov.convert_model(\n",
    "        model.visual,\n",
    "        example_input=dummy_input\n",
    "    )\n",
    "    \n",
    "    ov.save_model(vision_ov_model, VISION_MODEL_PATH)\n",
    "    print(f\"✓ Vision model saved to {VISION_MODEL_PATH}\")\n",
    "else:\n",
    "    print(\"✓ Vision model already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b6bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Text encoder already exists\n"
     ]
    }
   ],
   "source": [
    "if not TEXT_MODEL_PATH.exists():\n",
    "    print(\"Converting text encoder to OpenVINO...\")\n",
    "    \n",
    "    # Create dummy input for text model (tokenized text)\n",
    "    dummy_text = clip.tokenize([\"a photo of a cat\"])\n",
    "    \n",
    "    # We need to create a wrapper for the text encoding part\n",
    "    class CLIPTextEncoder(torch.nn.Module):\n",
    "        def __init__(self, clip_model):\n",
    "            super().__init__()\n",
    "            self.clip_model = clip_model\n",
    "        \n",
    "        def forward(self, text):\n",
    "            return self.clip_model.encode_text(text)\n",
    "    \n",
    "    text_encoder = CLIPTextEncoder(model)\n",
    "    text_encoder.eval()\n",
    "    \n",
    "    import openvino as ov\n",
    "    text_ov_model = ov.convert_model(\n",
    "        text_encoder,\n",
    "        example_input=dummy_text\n",
    "    )\n",
    "    \n",
    "    ov.save_model(text_ov_model, TEXT_MODEL_PATH)\n",
    "    print(f\"✓ Text encoder saved to {TEXT_MODEL_PATH}\")\n",
    "else:\n",
    "    print(\"✓ Text encoder already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a387579",
   "metadata": {},
   "source": [
    "## Initialize OpenVINO Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c9c656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vision model for inference...\n",
      "✓ Vision model loaded on CPU\n",
      "Input shape: [?,3,?,?]\n",
      "Output shape: [?,512]\n",
      "✓ Vision model loaded on CPU\n",
      "Input shape: [?,3,?,?]\n",
      "Output shape: [?,512]\n"
     ]
    }
   ],
   "source": [
    "core = Core()\n",
    "\n",
    "print(\"Loading vision model for inference...\")\n",
    "vision_compiled_model = core.compile_model(str(VISION_MODEL_PATH), \"CPU\")\n",
    "vision_input_layer = vision_compiled_model.input(0)\n",
    "vision_output_layer = vision_compiled_model.output(0)\n",
    "\n",
    "print(\"✓ Vision model loaded on CPU\")\n",
    "print(f\"Input shape: {vision_input_layer.partial_shape}\")\n",
    "print(f\"Output shape: {vision_output_layer.partial_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6919f865",
   "metadata": {},
   "source": [
    "## Collect Dataset Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing embeddings found. Will generate new embeddings...\n",
      "Found 26179 images in dataset\n"
     ]
    }
   ],
   "source": [
    "# Check if embeddings already exist\n",
    "if EMBEDDINGS_FILE.exists() and METADATA_FILE.exists():\n",
    "    print(\"✓ Embeddings database found! Loading existing embeddings...\")\n",
    "    \n",
    "    # Load embeddings\n",
    "    embeddings = np.load(EMBEDDINGS_FILE)\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(METADATA_FILE, 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    valid_image_paths = metadata['image_paths']\n",
    "    \n",
    "    print(f\"✓ Loaded {len(embeddings)} embeddings with shape {embeddings.shape}\")\n",
    "    print(f\"✓ Total images: {metadata['total_images']}\")\n",
    "    print(f\"✓ Embedding dimension: {metadata['embedding_dim']}\")\n",
    "    \n",
    "    # Set flag to skip generation\n",
    "    skip_generation = True\n",
    "else:\n",
    "    print(\"No existing embeddings found. Will generate new embeddings...\")\n",
    "    skip_generation = False\n",
    "    \n",
    "    # Collect all image paths\n",
    "    image_paths = []\n",
    "    supported_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.webp'}\n",
    "\n",
    "    for category_dir in DATASET_PATH.iterdir():\n",
    "        if category_dir.is_dir():\n",
    "            for img_path in category_dir.iterdir():\n",
    "                if img_path.suffix.lower() in supported_formats:\n",
    "                    image_paths.append(str(img_path))\n",
    "\n",
    "    print(f\"Found {len(image_paths)} images in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11c39d",
   "metadata": {},
   "source": [
    "## Generate Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825fe669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image_path):\n",
    "    \"\"\"Generate embedding for a single image using OpenVINO CLIP vision model\"\"\"\n",
    "    try:\n",
    "        # Load and preprocess image using CLIP's preprocessing\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = preprocess(image).unsqueeze(0)\n",
    "        \n",
    "        # Run inference with OpenVINO\n",
    "        pixel_values = image_tensor.numpy()\n",
    "        result = vision_compiled_model([pixel_values])[vision_output_layer]\n",
    "        \n",
    "        # Get the embedding and normalize\n",
    "        embedding = result[0]\n",
    "        embedding = embedding / np.linalg.norm(embedding)\n",
    "        \n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5fc571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 26179/26179 [23:11<00:00, 18.82it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 26179 embeddings with shape (26179, 512)\n",
      "✓ Embeddings saved to e:\\Projects\\AI Based\\RecTrio\\V2\\vector_db\\embeddings.npy\n",
      "✓ Metadata saved to e:\\Projects\\AI Based\\RecTrio\\V2\\vector_db\\metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "if not skip_generation:\n",
    "    embeddings = []\n",
    "    valid_image_paths = []\n",
    "\n",
    "    print(\"Generating embeddings...\")\n",
    "    for img_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "        embedding = get_image_embedding(img_path)\n",
    "        if embedding is not None:\n",
    "            embeddings.append(embedding)\n",
    "            valid_image_paths.append(img_path)\n",
    "\n",
    "    embeddings = np.array(embeddings).astype('float32')\n",
    "    print(f\"Generated {len(embeddings)} embeddings with shape {embeddings.shape}\")\n",
    "    \n",
    "    # Save embeddings to file\n",
    "    np.save(EMBEDDINGS_FILE, embeddings)\n",
    "    print(f\"✓ Embeddings saved to {EMBEDDINGS_FILE}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'image_paths': valid_image_paths,\n",
    "        'total_images': len(valid_image_paths),\n",
    "        'embedding_dim': embeddings.shape[1]\n",
    "    }\n",
    "    \n",
    "    with open(METADATA_FILE, 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    print(f\"✓ Metadata saved to {METADATA_FILE}\")\n",
    "else:\n",
    "    print(\"✓ Skipping embedding generation (already loaded from disk)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb617c",
   "metadata": {},
   "source": [
    "## Build FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5c767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS index...\n",
      "✓ FAISS index built with 26179 vectors\n",
      "✓ FAISS index saved to e:\\Projects\\AI Based\\RecTrio\\V2\\vector_db\\faiss_index.bin\n"
     ]
    }
   ],
   "source": [
    "if not FAISS_INDEX_FILE.exists():\n",
    "    print(\"Building FAISS index...\")\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    \n",
    "    index.add(embeddings)\n",
    "    print(f\"✓ FAISS index built with {index.ntotal} vectors\")\n",
    "    \n",
    "    # Save FAISS index\n",
    "    faiss.write_index(index, str(FAISS_INDEX_FILE))\n",
    "    print(f\"✓ FAISS index saved to {FAISS_INDEX_FILE}\")\n",
    "else:\n",
    "    print(\"✓ FAISS index already exists, loading...\")\n",
    "    index = faiss.read_index(str(FAISS_INDEX_FILE))\n",
    "    print(f\"✓ FAISS index loaded with {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d35270",
   "metadata": {},
   "source": [
    "## Save Index and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7050cb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EMBEDDING DATABASE SUMMARY\n",
      "============================================================\n",
      "Total images indexed: 26179\n",
      "Embedding dimension: 512\n",
      "FAISS index size: 26179 vectors\n",
      "\n",
      "Files saved in: e:\\Projects\\AI Based\\RecTrio\\V2\\vector_db\n",
      "  - Embeddings: embeddings.npy\n",
      "  - Metadata: metadata.pkl\n",
      "  - FAISS Index: faiss_index.bin\n",
      "============================================================\n",
      "\n",
      "✓ Embedding database ready for inference!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EMBEDDING DATABASE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total images indexed: {len(valid_image_paths)}\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"FAISS index size: {index.ntotal} vectors\")\n",
    "print(f\"\\nFiles saved in: {VECTOR_DB_DIR}\")\n",
    "print(f\"  - Embeddings: {EMBEDDINGS_FILE.name}\")\n",
    "print(f\"  - Metadata: {METADATA_FILE.name}\")\n",
    "print(f\"  - FAISS Index: {FAISS_INDEX_FILE.name}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n✓ Embedding database ready for inference!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
