{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c24c8a3",
   "metadata": {},
   "source": [
    "# Build CLIP Embeddings with OpenVINO\n",
    "\n",
    "This notebook converts CLIP model to OpenVINO format and builds embeddings database for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47a33665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\sanjay nithin\\appdata\\local\\temp\\pip-req-build-vdjqjke8\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from clip==1.0) (25.0)\n",
      "Requirement already satisfied: regex in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from clip==1.0) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from clip==1.0) (2.9.1)\n",
      "Requirement already satisfied: torchvision in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from clip==1.0) (0.24.1)\n",
      "Requirement already satisfied: wcwidth in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from ftfy->clip==1.0) (0.2.14)\n",
      "Requirement already satisfied: filelock in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (2025.12.0)\n",
      "Requirement already satisfied: numpy in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torchvision->clip==1.0) (12.0.0)\n",
      "Requirement already satisfied: colorama in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from jinja2->torch->clip==1.0) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\Sanjay Nithin\\AppData\\Local\\Temp\\pip-req-build-vdjqjke8'\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openvino in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (2024.6.0)\n",
      "Requirement already satisfied: faiss-cpu in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (1.13.2)\n",
      "Requirement already satisfied: pillow in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (12.0.0)\n",
      "Requirement already satisfied: tqdm in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.16.6 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from openvino) (1.26.4)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.2.1 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from openvino) (2025.2.0)\n",
      "Requirement already satisfied: packaging in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from openvino) (25.0)\n",
      "Requirement already satisfied: colorama in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://github.com/openai/CLIP.git\n",
    "! pip install openvino faiss-cpu pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b3ffe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "from openvino.runtime import Core\n",
    "import faiss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b0744",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe4f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = Path(r\"e:\\Projects\\AI Based\\RecTrio\\datasets\\animals\\raw-img\")\n",
    "MODEL_DIR = Path(r\"e:\\Projects\\AI Based\\RecTrio\\V2\\models\")\n",
    "VECTOR_DB_DIR = Path(r\"e:\\Projects\\AI Based\\RecTrio\\V2\\vector_db\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CLIP_MODEL_NAME = \"ViT-B/32\"  # Fast GitHub CLIP model\n",
    "VISION_MODEL_PATH = MODEL_DIR / \"clip_vision_model.xml\"\n",
    "TEXT_MODEL_PATH = MODEL_DIR / \"clip_text_model.xml\"\n",
    "\n",
    "# Embedding database files\n",
    "EMBEDDINGS_FILE = VECTOR_DB_DIR / \"embeddings.npy\"\n",
    "METADATA_FILE = VECTOR_DB_DIR / \"metadata.pkl\"\n",
    "FAISS_INDEX_FILE = VECTOR_DB_DIR / \"faiss_index.bin\"\n",
    "\n",
    "print(f\"✓ Model directory: {MODEL_DIR}\")\n",
    "print(f\"✓ Vector DB directory: {VECTOR_DB_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6afa48",
   "metadata": {},
   "source": [
    "## Load and Convert CLIP Model to OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f1ae19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model from GitHub (fast loading)...\n",
      "✓ CLIP ViT-B/32 loaded successfully\n",
      "Image input size: 224x224\n",
      "Embedding dimension: 512\n",
      "✓ CLIP ViT-B/32 loaded successfully\n",
      "Image input size: 224x224\n",
      "Embedding dimension: 512\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading CLIP model from GitHub (fast loading)...\")\n",
    "model, preprocess = clip.load(CLIP_MODEL_NAME, device=\"cpu\")\n",
    "model.eval()\n",
    "print(f\"✓ CLIP {CLIP_MODEL_NAME} loaded successfully\")\n",
    "print(f\"Image input size: 224x224\")\n",
    "print(f\"Embedding dimension: 512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26baccd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Vision model already exists\n"
     ]
    }
   ],
   "source": [
    "if not VISION_MODEL_PATH.exists():\n",
    "    print(\"Converting vision model to OpenVINO...\")\n",
    "    \n",
    "    # Create dummy input for vision model\n",
    "    dummy_input = torch.randn(1, 3, 224, 224)\n",
    "    \n",
    "    import openvino as ov\n",
    "    vision_ov_model = ov.convert_model(\n",
    "        model.visual,\n",
    "        example_input=dummy_input\n",
    "    )\n",
    "    \n",
    "    ov.save_model(vision_ov_model, VISION_MODEL_PATH)\n",
    "    print(f\"✓ Vision model saved to {VISION_MODEL_PATH}\")\n",
    "else:\n",
    "    print(\"✓ Vision model already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "187b6bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Text encoder already exists\n"
     ]
    }
   ],
   "source": [
    "if not TEXT_MODEL_PATH.exists():\n",
    "    print(\"Converting text encoder to OpenVINO...\")\n",
    "    \n",
    "    # Create dummy input for text model (tokenized text)\n",
    "    dummy_text = clip.tokenize([\"a photo of a cat\"])\n",
    "    \n",
    "    # We need to create a wrapper for the text encoding part\n",
    "    class CLIPTextEncoder(torch.nn.Module):\n",
    "        def __init__(self, clip_model):\n",
    "            super().__init__()\n",
    "            self.clip_model = clip_model\n",
    "        \n",
    "        def forward(self, text):\n",
    "            return self.clip_model.encode_text(text)\n",
    "    \n",
    "    text_encoder = CLIPTextEncoder(model)\n",
    "    text_encoder.eval()\n",
    "    \n",
    "    import openvino as ov\n",
    "    text_ov_model = ov.convert_model(\n",
    "        text_encoder,\n",
    "        example_input=dummy_text\n",
    "    )\n",
    "    \n",
    "    ov.save_model(text_ov_model, TEXT_MODEL_PATH)\n",
    "    print(f\"✓ Text encoder saved to {TEXT_MODEL_PATH}\")\n",
    "else:\n",
    "    print(\"✓ Text encoder already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a387579",
   "metadata": {},
   "source": [
    "## Initialize OpenVINO Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01c9c656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vision model for inference...\n",
      "✓ Vision model loaded on CPU\n",
      "Input shape: [?,3,?,?]\n",
      "Output shape: [?,512]\n",
      "✓ Vision model loaded on CPU\n",
      "Input shape: [?,3,?,?]\n",
      "Output shape: [?,512]\n"
     ]
    }
   ],
   "source": [
    "core = Core()\n",
    "\n",
    "print(\"Loading vision model for inference...\")\n",
    "vision_compiled_model = core.compile_model(str(VISION_MODEL_PATH), \"CPU\")\n",
    "vision_input_layer = vision_compiled_model.input(0)\n",
    "vision_output_layer = vision_compiled_model.output(0)\n",
    "\n",
    "print(\"✓ Vision model loaded on CPU\")\n",
    "print(f\"Input shape: {vision_input_layer.partial_shape}\")\n",
    "print(f\"Output shape: {vision_output_layer.partial_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6919f865",
   "metadata": {},
   "source": [
    "## Collect Dataset Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2bf761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing embeddings found. Will generate new embeddings...\n",
      "Found 26179 images in dataset\n"
     ]
    }
   ],
   "source": [
    "# Check if embeddings already exist\n",
    "if EMBEDDINGS_FILE.exists() and METADATA_FILE.exists():\n",
    "    print(\"✓ Embeddings database found! Loading existing embeddings...\")\n",
    "    \n",
    "    # Load embeddings\n",
    "    embeddings = np.load(EMBEDDINGS_FILE)\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(METADATA_FILE, 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    valid_image_paths = metadata['image_paths']\n",
    "    \n",
    "    print(f\"✓ Loaded {len(embeddings)} embeddings with shape {embeddings.shape}\")\n",
    "    print(f\"✓ Total images: {metadata['total_images']}\")\n",
    "    print(f\"✓ Embedding dimension: {metadata['embedding_dim']}\")\n",
    "    \n",
    "    # Set flag to skip generation\n",
    "    skip_generation = True\n",
    "else:\n",
    "    print(\"No existing embeddings found. Will generate new embeddings...\")\n",
    "    skip_generation = False\n",
    "    \n",
    "    # Collect all image paths\n",
    "    image_paths = []\n",
    "    supported_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.webp'}\n",
    "\n",
    "    for category_dir in DATASET_PATH.iterdir():\n",
    "        if category_dir.is_dir():\n",
    "            for img_path in category_dir.iterdir():\n",
    "                if img_path.suffix.lower() in supported_formats:\n",
    "                    image_paths.append(str(img_path))\n",
    "\n",
    "    print(f\"Found {len(image_paths)} images in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11c39d",
   "metadata": {},
   "source": [
    "## Generate Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "825fe669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image_path):\n",
    "    \"\"\"Generate embedding for a single image using OpenVINO CLIP vision model\"\"\"\n",
    "    try:\n",
    "        # Load and preprocess image using CLIP's preprocessing\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = preprocess(image).unsqueeze(0)\n",
    "        \n",
    "        # Run inference with OpenVINO\n",
    "        pixel_values = image_tensor.numpy()\n",
    "        result = vision_compiled_model([pixel_values])[vision_output_layer]\n",
    "        \n",
    "        # Get the embedding and normalize\n",
    "        embedding = result[0]\n",
    "        embedding = embedding / np.linalg.norm(embedding)\n",
    "        \n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e5fc571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 26179/26179 [23:11<00:00, 18.82it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 26179 embeddings with shape (26179, 512)\n",
      "✓ Embeddings saved to e:\\Projects\\AI Based\\RecTrio\\V2\\vector_db\\embeddings.npy\n",
      "✓ Metadata saved to e:\\Projects\\AI Based\\RecTrio\\V2\\vector_db\\metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "if not skip_generation:\n",
    "    embeddings = []\n",
    "    valid_image_paths = []\n",
    "\n",
    "    print(\"Generating embeddings...\")\n",
    "    for img_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "        embedding = get_image_embedding(img_path)\n",
    "        if embedding is not None:\n",
    "            embeddings.append(embedding)\n",
    "            valid_image_paths.append(img_path)\n",
    "\n",
    "    embeddings = np.array(embeddings).astype('float32')\n",
    "    print(f\"Generated {len(embeddings)} embeddings with shape {embeddings.shape}\")\n",
    "    \n",
    "    # Save embeddings to file\n",
    "    np.save(EMBEDDINGS_FILE, embeddings)\n",
    "    print(f\"✓ Embeddings saved to {EMBEDDINGS_FILE}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'image_paths': valid_image_paths,\n",
    "        'total_images': len(valid_image_paths),\n",
    "        'embedding_dim': embeddings.shape[1]\n",
    "    }\n",
    "    \n",
    "    with open(METADATA_FILE, 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    print(f\"✓ Metadata saved to {METADATA_FILE}\")\n",
    "else:\n",
    "    print(\"✓ Skipping embedding generation (already loaded from disk)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb617c",
   "metadata": {},
   "source": [
    "## Build FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eeb5c767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS index...\n",
      "✓ FAISS index built with 26179 vectors\n",
      "✓ FAISS index saved to e:\\Projects\\AI Based\\RecTrio\\V2\\vector_db\\faiss_index.bin\n"
     ]
    }
   ],
   "source": [
    "if not FAISS_INDEX_FILE.exists():\n",
    "    print(\"Building FAISS index...\")\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    \n",
    "    index.add(embeddings)\n",
    "    print(f\"✓ FAISS index built with {index.ntotal} vectors\")\n",
    "    \n",
    "    # Save FAISS index\n",
    "    faiss.write_index(index, str(FAISS_INDEX_FILE))\n",
    "    print(f\"✓ FAISS index saved to {FAISS_INDEX_FILE}\")\n",
    "else:\n",
    "    print(\"✓ FAISS index already exists, loading...\")\n",
    "    index = faiss.read_index(str(FAISS_INDEX_FILE))\n",
    "    print(f\"✓ FAISS index loaded with {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d35270",
   "metadata": {},
   "source": [
    "## Save Index and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7050cb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EMBEDDING DATABASE SUMMARY\n",
      "============================================================\n",
      "Total images indexed: 26179\n",
      "Embedding dimension: 512\n",
      "FAISS index size: 26179 vectors\n",
      "\n",
      "Files saved in: e:\\Projects\\AI Based\\RecTrio\\V2\\vector_db\n",
      "  - Embeddings: embeddings.npy\n",
      "  - Metadata: metadata.pkl\n",
      "  - FAISS Index: faiss_index.bin\n",
      "============================================================\n",
      "\n",
      "✓ Embedding database ready for inference!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EMBEDDING DATABASE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total images indexed: {len(valid_image_paths)}\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"FAISS index size: {index.ntotal} vectors\")\n",
    "print(f\"\\nFiles saved in: {VECTOR_DB_DIR}\")\n",
    "print(f\"  - Embeddings: {EMBEDDINGS_FILE.name}\")\n",
    "print(f\"  - Metadata: {METADATA_FILE.name}\")\n",
    "print(f\"  - FAISS Index: {FAISS_INDEX_FILE.name}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n✓ Embedding database ready for inference!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
