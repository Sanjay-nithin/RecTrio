{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c24c8a3",
   "metadata": {},
   "source": [
    "# Build CLIP Embeddings with OpenVINO\n",
    "\n",
    "This notebook converts CLIP model to OpenVINO format and builds embeddings database for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a33665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\sanjay nithin\\appdata\\local\\temp\\pip-req-build-8ci2kg1c\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting ftfy (from clip==1.0)\n",
      "  Obtaining dependency information for ftfy from https://files.pythonhosted.org/packages/ab/6e/81d47999aebc1b155f81eca4477a616a70f238a2549848c38983f3c22a82/ftfy-6.3.1-py3-none-any.whl.metadata\n",
      "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: packaging in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from clip==1.0) (25.0)\n",
      "Requirement already satisfied: regex in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from clip==1.0) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from clip==1.0) (2.9.1)\n",
      "Collecting torchvision (from clip==1.0)\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/fa/bb/cfc6a6f6ccc84a534ed1fdf029ae5716dd6ff04e57ed9dc2dab38bf652d5/torchvision-0.24.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached torchvision-0.24.1-cp311-cp311-win_amd64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: wcwidth in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from ftfy->clip==1.0) (0.2.14)\n",
      "Requirement already satisfied: filelock in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torch->clip==1.0) (2025.12.0)\n",
      "Requirement already satisfied: numpy in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from torchvision->clip==1.0) (12.0.0)\n",
      "Requirement already satisfied: colorama in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
      "Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Using cached torchvision-0.24.1-cp311-cp311-win_amd64.whl (4.0 MB)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (pyproject.toml): started\n",
      "  Building wheel for clip (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369633 sha256=4b624c03cebd97780571b87772e5652ad1b11f8fb5113e79170bf61b44e99dab\n",
      "  Stored in directory: C:\\Users\\Sanjay Nithin\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-j5afarcw\\wheels\\3f\\7c\\a4\\9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
      "Successfully built clip\n",
      "Installing collected packages: ftfy, torchvision, clip\n",
      "Successfully installed clip-1.0 ftfy-6.3.1 torchvision-0.24.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\Sanjay Nithin\\AppData\\Local\\Temp\\pip-req-build-8ci2kg1c'\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openvino in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (2024.6.0)\n",
      "Requirement already satisfied: faiss-cpu in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (1.13.2)\n",
      "Requirement already satisfied: pillow in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (12.0.0)\n",
      "Requirement already satisfied: tqdm in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.16.6 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from openvino) (1.26.4)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.2.1 in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from openvino) (2025.2.0)\n",
      "Requirement already satisfied: packaging in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from openvino) (25.0)\n",
      "Requirement already satisfied: colorama in e:\\projects\\ai based\\rectrio\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://github.com/openai/CLIP.git\n",
    "! pip install openvino faiss-cpu pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3ffe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "from openvino.runtime import Core\n",
    "import faiss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b0744",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe4f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = Path(r\"e:\\Projects\\AI Based\\RecTrio\\V1\\datasets\\animals\\raw-img\")\n",
    "MODEL_DIR = Path(r\"e:\\Projects\\AI Based\\RecTrio\\V2\\models\")\n",
    "VECTOR_DB_DIR = Path(r\"e:\\Projects\\AI Based\\RecTrio\\V2\\vector_db\")\n",
    "\n",
    "CLIP_MODEL_NAME = \"ViT-B/32\"  # Fast GitHub CLIP model\n",
    "VISION_MODEL_PATH = MODEL_DIR / \"clip_vision_model.xml\"\n",
    "TEXT_MODEL_PATH = MODEL_DIR / \"clip_text_model.xml\"\n",
    "\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6afa48",
   "metadata": {},
   "source": [
    "## Load and Convert CLIP Model to OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f1ae19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model from GitHub (fast loading)...\n",
      "✓ CLIP ViT-B/32 loaded successfully\n",
      "Image input size: 224x224\n",
      "Embedding dimension: 512\n",
      "✓ CLIP ViT-B/32 loaded successfully\n",
      "Image input size: 224x224\n",
      "Embedding dimension: 512\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading CLIP model from GitHub (fast loading)...\")\n",
    "model, preprocess = clip.load(CLIP_MODEL_NAME, device=\"cpu\")\n",
    "model.eval()\n",
    "print(f\"✓ CLIP {CLIP_MODEL_NAME} loaded successfully\")\n",
    "print(f\"Image input size: 224x224\")\n",
    "print(f\"Embedding dimension: 512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26baccd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting vision model to OpenVINO...\n",
      "✓ Vision model saved to e:\\Projects\\AI Based\\RecTrio\\V2\\models\\clip_vision_model.xml\n",
      "✓ Vision model saved to e:\\Projects\\AI Based\\RecTrio\\V2\\models\\clip_vision_model.xml\n"
     ]
    }
   ],
   "source": [
    "if not VISION_MODEL_PATH.exists():\n",
    "    print(\"Converting vision model to OpenVINO...\")\n",
    "    \n",
    "    # Create dummy input for vision model\n",
    "    dummy_input = torch.randn(1, 3, 224, 224)\n",
    "    \n",
    "    import openvino as ov\n",
    "    vision_ov_model = ov.convert_model(\n",
    "        model.visual,\n",
    "        example_input=dummy_input\n",
    "    )\n",
    "    \n",
    "    ov.save_model(vision_ov_model, VISION_MODEL_PATH)\n",
    "    print(f\"✓ Vision model saved to {VISION_MODEL_PATH}\")\n",
    "else:\n",
    "    print(\"✓ Vision model already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "187b6bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting text encoder to OpenVINO...\n",
      "✓ Text encoder saved to e:\\Projects\\AI Based\\RecTrio\\V2\\models\\clip_text_model.xml\n",
      "✓ Text encoder saved to e:\\Projects\\AI Based\\RecTrio\\V2\\models\\clip_text_model.xml\n"
     ]
    }
   ],
   "source": [
    "if not TEXT_MODEL_PATH.exists():\n",
    "    print(\"Converting text encoder to OpenVINO...\")\n",
    "    \n",
    "    # Create dummy input for text model (tokenized text)\n",
    "    dummy_text = clip.tokenize([\"a photo of a cat\"])\n",
    "    \n",
    "    # We need to create a wrapper for the text encoding part\n",
    "    class CLIPTextEncoder(torch.nn.Module):\n",
    "        def __init__(self, clip_model):\n",
    "            super().__init__()\n",
    "            self.clip_model = clip_model\n",
    "        \n",
    "        def forward(self, text):\n",
    "            return self.clip_model.encode_text(text)\n",
    "    \n",
    "    text_encoder = CLIPTextEncoder(model)\n",
    "    text_encoder.eval()\n",
    "    \n",
    "    import openvino as ov\n",
    "    text_ov_model = ov.convert_model(\n",
    "        text_encoder,\n",
    "        example_input=dummy_text\n",
    "    )\n",
    "    \n",
    "    ov.save_model(text_ov_model, TEXT_MODEL_PATH)\n",
    "    print(f\"✓ Text encoder saved to {TEXT_MODEL_PATH}\")\n",
    "else:\n",
    "    print(\"✓ Text encoder already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a387579",
   "metadata": {},
   "source": [
    "## Initialize OpenVINO Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01c9c656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vision model for inference...\n",
      "✓ Vision model loaded on CPU\n",
      "Input shape: [?,3,?,?]\n",
      "Output shape: [?,512]\n",
      "✓ Vision model loaded on CPU\n",
      "Input shape: [?,3,?,?]\n",
      "Output shape: [?,512]\n"
     ]
    }
   ],
   "source": [
    "core = Core()\n",
    "\n",
    "print(\"Loading vision model for inference...\")\n",
    "vision_compiled_model = core.compile_model(str(VISION_MODEL_PATH), \"CPU\")\n",
    "vision_input_layer = vision_compiled_model.input(0)\n",
    "vision_output_layer = vision_compiled_model.output(0)\n",
    "\n",
    "print(\"✓ Vision model loaded on CPU\")\n",
    "print(f\"Input shape: {vision_input_layer.partial_shape}\")\n",
    "print(f\"Output shape: {vision_output_layer.partial_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6919f865",
   "metadata": {},
   "source": [
    "## Collect Dataset Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2bf761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26179 images in dataset\n"
     ]
    }
   ],
   "source": [
    "image_paths = []\n",
    "supported_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.webp'}\n",
    "\n",
    "for category_dir in DATASET_PATH.iterdir():\n",
    "    if category_dir.is_dir():\n",
    "        for img_path in category_dir.iterdir():\n",
    "            if img_path.suffix.lower() in supported_formats:\n",
    "                image_paths.append(str(img_path))\n",
    "\n",
    "print(f\"Found {len(image_paths)} images in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11c39d",
   "metadata": {},
   "source": [
    "## Generate Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "825fe669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image_path):\n",
    "    \"\"\"Generate embedding for a single image using OpenVINO CLIP vision model\"\"\"\n",
    "    try:\n",
    "        # Load and preprocess image using CLIP's preprocessing\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = preprocess(image).unsqueeze(0)\n",
    "        \n",
    "        # Run inference with OpenVINO\n",
    "        pixel_values = image_tensor.numpy()\n",
    "        result = vision_compiled_model([pixel_values])[vision_output_layer]\n",
    "        \n",
    "        # Get the embedding and normalize\n",
    "        embedding = result[0]\n",
    "        embedding = embedding / np.linalg.norm(embedding)\n",
    "        \n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5fc571",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "valid_image_paths = []\n",
    "\n",
    "print(\"Generating embeddings...\")\n",
    "for img_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "    embedding = get_image_embedding(img_path)\n",
    "    if embedding is not None:\n",
    "        embeddings.append(embedding)\n",
    "        valid_image_paths.append(img_path)\n",
    "\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "print(f\"Generated {len(embeddings)} embeddings with shape {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb617c",
   "metadata": {},
   "source": [
    "## Build FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "index.add(embeddings)\n",
    "print(f\"FAISS index built with {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d35270",
   "metadata": {},
   "source": [
    "## Save Index and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7050cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index_path = VECTOR_DB_DIR / \"faiss_index.bin\"\n",
    "faiss.write_index(index, str(faiss_index_path))\n",
    "print(f\"FAISS index saved to {faiss_index_path}\")\n",
    "\n",
    "metadata = {\n",
    "    'image_paths': valid_image_paths,\n",
    "    'total_images': len(valid_image_paths),\n",
    "    'embedding_dim': dimension\n",
    "}\n",
    "\n",
    "metadata_path = VECTOR_DB_DIR / \"metadata.pkl\"\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"Metadata saved to {metadata_path}\")\n",
    "\n",
    "print(\"\\nEmbedding database built successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
