{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acbbeb1c",
   "metadata": {},
   "source": [
    "# VisionRec V2 - Multimodal Training Pipeline\n",
    "\n",
    "This notebook implements the training pipeline for a custom multimodal embedding model that learns a shared embedding space for both images and text using triplet loss.\n",
    "\n",
    "## Architecture Overview\n",
    "- **Dual Encoder**: Separate encoders for images and text\n",
    "- **Image Encoder**: ResNet-50 backbone with projection head\n",
    "- **Text Encoder**: Transformer-based encoder with projection head\n",
    "- **Loss Function**: Triplet Loss with semi-hard negative mining\n",
    "- **Triplet Types**: Image-Image, Text-Image, Image-Text\n",
    "\n",
    "## Training Strategy\n",
    "1. Load and preprocess multimodal dataset\n",
    "2. Create triplets with semi-hard negative mining\n",
    "3. Train dual encoders to produce normalized embeddings\n",
    "4. Validate on test set\n",
    "5. Save trained model weights and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef4578",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies\n",
    "\n",
    "Import all required libraries for dataset handling, model building, training, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9114daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import List, Tuple, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration (CPU-friendly)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64839a83",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Define all hyperparameters and paths for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff770bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    DATASET_ROOT = r\"E:\\Projects\\AI Based\\RecTrio\\datasets\\animals\\raw-img\"\n",
    "    OUTPUT_DIR = r\".kaggle/working/outputs/model\"\n",
    "    \n",
    "    # Model architecture\n",
    "    EMBEDDING_DIM = 512\n",
    "    IMAGE_SIZE = 224\n",
    "    TEXT_MAX_LENGTH = 50\n",
    "    VOCAB_SIZE = 10000\n",
    "    TEXT_EMBED_DIM = 300\n",
    "    TEXT_HIDDEN_DIM = 256\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    \n",
    "    # Triplet loss\n",
    "    MARGIN = 0.5\n",
    "    MINING_STRATEGY = 'semi-hard'  # semi-hard negative mining\n",
    "    \n",
    "    # Data split\n",
    "    TRAIN_SPLIT = 0.8\n",
    "    VAL_SPLIT = 0.1\n",
    "    TEST_SPLIT = 0.1\n",
    "    \n",
    "    # Training\n",
    "    NUM_WORKERS = 2\n",
    "    SAVE_FREQ = 5  # Save model every N epochs\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Dataset: {config.DATASET_ROOT}\")\n",
    "print(f\"  Output: {config.OUTPUT_DIR}\")\n",
    "print(f\"  Embedding Dim: {config.EMBEDDING_DIM}\")\n",
    "print(f\"  Batch Size: {config.BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {config.NUM_EPOCHS}\")\n",
    "print(f\"  Learning Rate: {config.LEARNING_RATE}\")\n",
    "print(f\"  Margin: {config.MARGIN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e709b27",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation\n",
    "\n",
    "Build vocabulary and prepare the multimodal dataset with image paths, text labels, and class IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2661614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_root: str):\n",
    "    \"\"\"\n",
    "    Load dataset from directory structure.\n",
    "    Expected structure: dataset_root/class_name/image_files\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (image_path, text_label, class_id)\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    class_names = sorted([d for d in os.listdir(dataset_root) \n",
    "                         if os.path.isdir(os.path.join(dataset_root, d))])\n",
    "    \n",
    "    class_to_id = {name: idx for idx, name in enumerate(class_names)}\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(dataset_root, class_name)\n",
    "        class_id = class_to_id[class_name]\n",
    "        \n",
    "        for img_file in os.listdir(class_dir):\n",
    "            if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                img_path = os.path.join(class_dir, img_file)\n",
    "                dataset.append((img_path, class_name, class_id))\n",
    "    \n",
    "    return dataset, class_to_id, class_names\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "full_dataset, class_to_id, class_names = load_dataset(config.DATASET_ROOT)\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Total samples: {len(full_dataset)}\")\n",
    "print(f\"  Number of classes: {len(class_names)}\")\n",
    "print(f\"  Classes: {class_names}\")\n",
    "\n",
    "# Count samples per class\n",
    "class_counts = defaultdict(int)\n",
    "for _, label, _ in full_dataset:\n",
    "    class_counts[label] += 1\n",
    "\n",
    "print(\"\\nSamples per class:\")\n",
    "for class_name in class_names:\n",
    "    print(f\"  {class_name}: {class_counts[class_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b7709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from class names\n",
    "class Vocabulary:\n",
    "    def __init__(self, max_vocab_size=10000):\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.word_counts = defaultdict(int)\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        \n",
    "    def build_vocab(self, texts: List[str]):\n",
    "        \"\"\"Build vocabulary from text labels\"\"\"\n",
    "        # Count word frequencies\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                self.word_counts[word] += 1\n",
    "        \n",
    "        # Sort by frequency and take top words\n",
    "        sorted_words = sorted(self.word_counts.items(), \n",
    "                            key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Add to vocabulary (leave space for PAD and UNK)\n",
    "        for word, _ in sorted_words[:self.max_vocab_size - 2]:\n",
    "            idx = len(self.word2idx)\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "    \n",
    "    def encode(self, text: str, max_length: int) -> List[int]:\n",
    "        \"\"\"Convert text to token indices\"\"\"\n",
    "        words = text.lower().split()\n",
    "        tokens = [self.word2idx.get(word, self.word2idx['<UNK>']) \n",
    "                 for word in words]\n",
    "        \n",
    "        # Pad or truncate\n",
    "        if len(tokens) < max_length:\n",
    "            tokens += [self.word2idx['<PAD>']] * (max_length - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:max_length]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = Vocabulary(max_vocab_size=config.VOCAB_SIZE)\n",
    "all_labels = [label for _, label, _ in full_dataset]\n",
    "vocab.build_vocab(all_labels)\n",
    "\n",
    "print(f\"\\nVocabulary built:\")\n",
    "print(f\"  Vocabulary size: {len(vocab)}\")\n",
    "print(f\"  Sample words: {list(vocab.word2idx.keys())[:10]}\")\n",
    "\n",
    "# Update config with actual vocab size\n",
    "config.VOCAB_SIZE = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ed4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, validation, and test sets\n",
    "random.shuffle(full_dataset)\n",
    "\n",
    "train_size = int(config.TRAIN_SPLIT * len(full_dataset))\n",
    "val_size = int(config.VAL_SPLIT * len(full_dataset))\n",
    "\n",
    "train_data = full_dataset[:train_size]\n",
    "val_data = full_dataset[train_size:train_size + val_size]\n",
    "test_data = full_dataset[train_size + val_size:]\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"  Train: {len(train_data)} samples ({config.TRAIN_SPLIT * 100}%)\")\n",
    "print(f\"  Validation: {len(val_data)} samples ({config.VAL_SPLIT * 100}%)\")\n",
    "print(f\"  Test: {len(test_data)} samples ({config.TEST_SPLIT * 100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee666c7",
   "metadata": {},
   "source": [
    "## 4. Custom Dataset Class\n",
    "\n",
    "PyTorch Dataset class that handles multimodal data loading and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f45188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Multimodal dataset for image-text embedding learning.\n",
    "    Each sample contains: image, text tokens, and class ID.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Tuple], vocab: Vocabulary, \n",
    "                 transform=None, max_text_length=50):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.max_text_length = max_text_length\n",
    "        \n",
    "        # Group data by class for triplet sampling\n",
    "        self.class_to_indices = defaultdict(list)\n",
    "        for idx, (_, _, class_id) in enumerate(data):\n",
    "            self.class_to_indices[class_id].append(idx)\n",
    "        \n",
    "        self.classes = list(self.class_to_indices.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, text_label, class_id = self.data[idx]\n",
    "        \n",
    "        # Load and transform image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a blank image if loading fails\n",
    "            image = torch.zeros(3, 224, 224)\n",
    "        \n",
    "        # Encode text\n",
    "        text_tokens = self.vocab.encode(text_label, self.max_text_length)\n",
    "        text_tokens = torch.tensor(text_tokens, dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text_tokens,\n",
    "            'class_id': class_id,\n",
    "            'text_label': text_label\n",
    "        }\n",
    "    \n",
    "    def get_triplet(self, anchor_idx):\n",
    "        \"\"\"\n",
    "        Generate a triplet: (anchor, positive, negative)\n",
    "        Both anchor and positive have the same class, negative has different class.\n",
    "        \"\"\"\n",
    "        anchor_class = self.data[anchor_idx][2]\n",
    "        \n",
    "        # Get positive (same class, different sample)\n",
    "        positive_candidates = [i for i in self.class_to_indices[anchor_class] \n",
    "                              if i != anchor_idx]\n",
    "        if len(positive_candidates) > 0:\n",
    "            positive_idx = random.choice(positive_candidates)\n",
    "        else:\n",
    "            positive_idx = anchor_idx  # Fallback if only one sample in class\n",
    "        \n",
    "        # Get negative (different class)\n",
    "        negative_classes = [c for c in self.classes if c != anchor_class]\n",
    "        negative_class = random.choice(negative_classes)\n",
    "        negative_idx = random.choice(self.class_to_indices[negative_class])\n",
    "        \n",
    "        return anchor_idx, positive_idx, negative_idx\n",
    "\n",
    "# Image transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(train_data, vocab, train_transform, \n",
    "                                 config.TEXT_MAX_LENGTH)\n",
    "val_dataset = MultimodalDataset(val_data, vocab, test_transform, \n",
    "                               config.TEXT_MAX_LENGTH)\n",
    "test_dataset = MultimodalDataset(test_data, vocab, test_transform, \n",
    "                                config.TEXT_MAX_LENGTH)\n",
    "\n",
    "print(f\"\\nDatasets created:\")\n",
    "print(f\"  Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"  Val dataset: {len(val_dataset)} samples\")\n",
    "print(f\"  Test dataset: {len(test_dataset)} samples\")\n",
    "\n",
    "# Test dataset\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample data:\")\n",
    "print(f\"  Image shape: {sample['image'].shape}\")\n",
    "print(f\"  Text tokens shape: {sample['text'].shape}\")\n",
    "print(f\"  Class ID: {sample['class_id']}\")\n",
    "print(f\"  Text label: {sample['text_label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7209a018",
   "metadata": {},
   "source": [
    "## 5. Model Architecture\n",
    "\n",
    "Dual encoder architecture with separate encoders for images and text, both projecting to a shared embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bdd0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Image encoder using ResNet-50 backbone with custom projection head.\n",
    "    Outputs L2-normalized embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=512):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet-50\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Remove final FC layer\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "        # Projection head\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, embedding_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.backbone(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        # Project to embedding space\n",
    "        embeddings = self.projection(features)\n",
    "        \n",
    "        # L2 normalize\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Text encoder using embedding layer + LSTM + projection head.\n",
    "    Outputs L2-normalized embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=300, hidden_dim=256, \n",
    "                 embedding_dim=512, num_layers=2):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                           batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        \n",
    "        # Projection head (bidirectional LSTM outputs hidden_dim * 2)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, embedding_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embed tokens\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # LSTM encoding\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Use mean pooling over sequence\n",
    "        text_features = torch.mean(lstm_out, dim=1)\n",
    "        \n",
    "        # Project to embedding space\n",
    "        embeddings = self.projection(text_features)\n",
    "        \n",
    "        # L2 normalize\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class DualEncoderModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual encoder model with shared embedding space for images and text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=512, \n",
    "                 text_embed_dim=300, text_hidden_dim=256):\n",
    "        super(DualEncoderModel, self).__init__()\n",
    "        \n",
    "        self.image_encoder = ImageEncoder(embedding_dim)\n",
    "        self.text_encoder = TextEncoder(vocab_size, text_embed_dim, \n",
    "                                       text_hidden_dim, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def forward(self, images, texts):\n",
    "        \"\"\"\n",
    "        Forward pass for both modalities.\n",
    "        \n",
    "        Args:\n",
    "            images: Batch of images [B, 3, H, W]\n",
    "            texts: Batch of text tokens [B, max_length]\n",
    "            \n",
    "        Returns:\n",
    "            image_embeddings: [B, embedding_dim]\n",
    "            text_embeddings: [B, embedding_dim]\n",
    "        \"\"\"\n",
    "        image_embeddings = self.image_encoder(images)\n",
    "        text_embeddings = self.text_encoder(texts)\n",
    "        \n",
    "        return image_embeddings, text_embeddings\n",
    "    \n",
    "    def encode_image(self, images):\n",
    "        \"\"\"Encode images only\"\"\"\n",
    "        return self.image_encoder(images)\n",
    "    \n",
    "    def encode_text(self, texts):\n",
    "        \"\"\"Encode texts only\"\"\"\n",
    "        return self.text_encoder(texts)\n",
    "\n",
    "# Initialize model\n",
    "model = DualEncoderModel(\n",
    "    vocab_size=config.VOCAB_SIZE,\n",
    "    embedding_dim=config.EMBEDDING_DIM,\n",
    "    text_embed_dim=config.TEXT_EMBED_DIM,\n",
    "    text_hidden_dim=config.TEXT_HIDDEN_DIM\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  Image Encoder: ResNet-50 + Projection Head\")\n",
    "print(f\"  Text Encoder: Embedding + BiLSTM + Projection Head\")\n",
    "print(f\"  Embedding Dimension: {config.EMBEDDING_DIM}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30cfbac",
   "metadata": {},
   "source": [
    "## 6. Triplet Loss with Semi-Hard Negative Mining\n",
    "\n",
    "Implement triplet loss function that supports Image-Image, Text-Image, and Image-Text triplets with semi-hard negative mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6981506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet Loss with online semi-hard negative mining.\n",
    "    \n",
    "    For a batch, computes all possible triplets and selects semi-hard negatives:\n",
    "    - Negative is farther than anchor-positive distance\n",
    "    - But within margin of the anchor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, margin=0.5):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def forward(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Compute triplet loss with semi-hard negative mining.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: [batch_size, embedding_dim] - L2 normalized\n",
    "            labels: [batch_size] - class labels\n",
    "            \n",
    "        Returns:\n",
    "            loss: scalar tensor\n",
    "            num_triplets: number of valid triplets\n",
    "        \"\"\"\n",
    "        # Compute pairwise distances (using dot product since normalized)\n",
    "        # Distance = 2 - 2 * cosine_similarity\n",
    "        dot_product = torch.matmul(embeddings, embeddings.t())\n",
    "        distances = 2.0 - 2.0 * dot_product\n",
    "        \n",
    "        # Create mask for positive pairs (same class)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        positive_mask = labels == labels.t()\n",
    "        positive_mask.fill_diagonal_(False)  # Exclude self\n",
    "        \n",
    "        # Create mask for negative pairs (different class)\n",
    "        negative_mask = labels != labels.t()\n",
    "        \n",
    "        # For each anchor, find hardest positive and semi-hard negative\n",
    "        losses = []\n",
    "        num_valid_triplets = 0\n",
    "        \n",
    "        for i in range(embeddings.size(0)):\n",
    "            # Get positive distances for this anchor\n",
    "            pos_dists = distances[i][positive_mask[i]]\n",
    "            if len(pos_dists) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Hardest positive (farthest positive)\n",
    "            hardest_positive_dist = pos_dists.max()\n",
    "            \n",
    "            # Get negative distances for this anchor\n",
    "            neg_dists = distances[i][negative_mask[i]]\n",
    "            if len(neg_dists) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Semi-hard negatives: farther than positive but within margin\n",
    "            semi_hard_negatives = neg_dists[\n",
    "                (neg_dists > hardest_positive_dist) & \n",
    "                (neg_dists < hardest_positive_dist + self.margin)\n",
    "            ]\n",
    "            \n",
    "            if len(semi_hard_negatives) > 0:\n",
    "                # Use hardest semi-hard negative\n",
    "                hardest_negative_dist = semi_hard_negatives.min()\n",
    "            else:\n",
    "                # If no semi-hard negative, use hardest negative\n",
    "                hardest_negative_dist = neg_dists.min()\n",
    "            \n",
    "            # Compute triplet loss\n",
    "            loss = F.relu(hardest_positive_dist - hardest_negative_dist + self.margin)\n",
    "            \n",
    "            if loss > 0:\n",
    "                losses.append(loss)\n",
    "                num_valid_triplets += 1\n",
    "        \n",
    "        if len(losses) == 0:\n",
    "            return torch.tensor(0.0, device=embeddings.device), 0\n",
    "        \n",
    "        return torch.stack(losses).mean(), num_valid_triplets\n",
    "\n",
    "\n",
    "def compute_multimodal_triplet_loss(model, batch, triplet_loss_fn, device):\n",
    "    \"\"\"\n",
    "    Compute triplet loss for multiple triplet types:\n",
    "    1. Image-Image triplets\n",
    "    2. Text-Image triplets (anchor: text, pos/neg: images)\n",
    "    3. Image-Text triplets (anchor: image, pos/neg: texts)\n",
    "    \n",
    "    Returns:\n",
    "        Combined loss and individual losses for logging\n",
    "    \"\"\"\n",
    "    images = batch['image'].to(device)\n",
    "    texts = batch['text'].to(device)\n",
    "    labels = batch['class_id'].to(device)\n",
    "    \n",
    "    # Encode both modalities\n",
    "    image_embeddings, text_embeddings = model(images, texts)\n",
    "    \n",
    "    # 1. Image-Image triplets\n",
    "    loss_img_img, num_img_img = triplet_loss_fn(image_embeddings, labels)\n",
    "    \n",
    "    # 2. Text-Text triplets\n",
    "    loss_txt_txt, num_txt_txt = triplet_loss_fn(text_embeddings, labels)\n",
    "    \n",
    "    # 3. Cross-modal: combine image and text embeddings for same class\n",
    "    # This encourages image and text of same class to be close\n",
    "    combined_embeddings = torch.cat([image_embeddings, text_embeddings], dim=0)\n",
    "    combined_labels = torch.cat([labels, labels], dim=0)\n",
    "    loss_cross_modal, num_cross = triplet_loss_fn(combined_embeddings, combined_labels)\n",
    "    \n",
    "    # Weighted combination\n",
    "    total_loss = (loss_img_img + loss_txt_txt + loss_cross_modal) / 3.0\n",
    "    \n",
    "    return total_loss, {\n",
    "        'loss_img_img': loss_img_img.item(),\n",
    "        'loss_txt_txt': loss_txt_txt.item(),\n",
    "        'loss_cross_modal': loss_cross_modal.item(),\n",
    "        'num_img_img': num_img_img,\n",
    "        'num_txt_txt': num_txt_txt,\n",
    "        'num_cross': num_cross\n",
    "    }\n",
    "\n",
    "# Initialize loss function\n",
    "triplet_loss_fn = TripletLoss(margin=config.MARGIN)\n",
    "\n",
    "print(f\"\\nTriplet Loss Configuration:\")\n",
    "print(f\"  Margin: {config.MARGIN}\")\n",
    "print(f\"  Mining Strategy: {config.MINING_STRATEGY}\")\n",
    "print(f\"  Triplet Types: Image-Image, Text-Text, Cross-Modal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c2f144",
   "metadata": {},
   "source": [
    "## 7. Training and Validation Functions\n",
    "\n",
    "Implement training loop with validation and metric computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d05d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_retrieval_accuracy(image_embeddings, text_embeddings, labels, k=5):\n",
    "    \"\"\"\n",
    "    Compute retrieval accuracy metrics:\n",
    "    - Image-to-Text: Given image, retrieve correct text in top-k\n",
    "    - Text-to-Image: Given text, retrieve correct image in top-k\n",
    "    \n",
    "    Args:\n",
    "        image_embeddings: [N, embedding_dim]\n",
    "        text_embeddings: [N, embedding_dim]\n",
    "        labels: [N] class labels\n",
    "        k: top-k accuracy\n",
    "        \n",
    "    Returns:\n",
    "        img2txt_acc: Image-to-text retrieval accuracy\n",
    "        txt2img_acc: Text-to-image retrieval accuracy\n",
    "    \"\"\"\n",
    "    # Compute similarity matrix (cosine similarity for normalized embeddings)\n",
    "    similarity = torch.matmul(image_embeddings, text_embeddings.t())\n",
    "    \n",
    "    # Image-to-Text retrieval\n",
    "    _, img2txt_indices = similarity.topk(k, dim=1)\n",
    "    img2txt_correct = 0\n",
    "    for i in range(len(labels)):\n",
    "        retrieved_labels = labels[img2txt_indices[i]]\n",
    "        if labels[i] in retrieved_labels:\n",
    "            img2txt_correct += 1\n",
    "    img2txt_acc = img2txt_correct / len(labels)\n",
    "    \n",
    "    # Text-to-Image retrieval\n",
    "    _, txt2img_indices = similarity.t().topk(k, dim=1)\n",
    "    txt2img_correct = 0\n",
    "    for i in range(len(labels)):\n",
    "        retrieved_labels = labels[txt2img_indices[i]]\n",
    "        if labels[i] in retrieved_labels:\n",
    "            txt2img_correct += 1\n",
    "    txt2img_acc = txt2img_correct / len(labels)\n",
    "    \n",
    "    return img2txt_acc, txt2img_acc\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, triplet_loss_fn, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_img_img_loss = 0\n",
    "    total_txt_txt_loss = 0\n",
    "    total_cross_modal_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch} [Train]\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, loss_dict = compute_multimodal_triplet_loss(\n",
    "            model, batch, triplet_loss_fn, device\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate losses\n",
    "        total_loss += loss.item()\n",
    "        total_img_img_loss += loss_dict['loss_img_img']\n",
    "        total_txt_txt_loss += loss_dict['loss_txt_txt']\n",
    "        total_cross_modal_loss += loss_dict['loss_cross_modal']\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'img_img': f\"{loss_dict['loss_img_img']:.4f}\",\n",
    "            'txt_txt': f\"{loss_dict['loss_txt_txt']:.4f}\",\n",
    "            'cross': f\"{loss_dict['loss_cross_modal']:.4f}\"\n",
    "        })\n",
    "    \n",
    "    # Compute averages\n",
    "    num_batches = len(dataloader)\n",
    "    return {\n",
    "        'loss': total_loss / num_batches,\n",
    "        'loss_img_img': total_img_img_loss / num_batches,\n",
    "        'loss_txt_txt': total_txt_txt_loss / num_batches,\n",
    "        'loss_cross_modal': total_cross_modal_loss / num_batches\n",
    "    }\n",
    "\n",
    "\n",
    "def validate(model, dataloader, triplet_loss_fn, device, epoch, split_name=\"Val\"):\n",
    "    \"\"\"Validate and compute retrieval accuracy\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch} [{split_name}]\")\n",
    "        \n",
    "        for batch in pbar:\n",
    "            # Compute loss\n",
    "            loss, loss_dict = compute_multimodal_triplet_loss(\n",
    "                model, batch, triplet_loss_fn, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Collect embeddings for retrieval accuracy\n",
    "            images = batch['image'].to(device)\n",
    "            texts = batch['text'].to(device)\n",
    "            labels = batch['class_id']\n",
    "            \n",
    "            image_emb, text_emb = model(images, texts)\n",
    "            \n",
    "            all_image_embeddings.append(image_emb.cpu())\n",
    "            all_text_embeddings.append(text_emb.cpu())\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # Concatenate all embeddings\n",
    "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    # Compute retrieval accuracy\n",
    "    img2txt_acc, txt2img_acc = compute_retrieval_accuracy(\n",
    "        all_image_embeddings, all_text_embeddings, all_labels, k=5\n",
    "    )\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'img2txt_acc': img2txt_acc,\n",
    "        'txt2img_acc': txt2img_acc,\n",
    "        'avg_acc': (img2txt_acc + txt2img_acc) / 2\n",
    "    }\n",
    "\n",
    "print(\"Training and validation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebabf72",
   "metadata": {},
   "source": [
    "## 8. Create Data Loaders\n",
    "\n",
    "Create PyTorch DataLoaders for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915603c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"\\nData Loaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "print(f\"  Batch size: {config.BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4651269",
   "metadata": {},
   "source": [
    "## 9. Optimizer and Scheduler\n",
    "\n",
    "Configure optimizer and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    weight_decay=config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Learning rate scheduler (reduce on plateau)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimizer: AdamW\")\n",
    "print(f\"  Learning Rate: {config.LEARNING_RATE}\")\n",
    "print(f\"  Weight Decay: {config.WEIGHT_DECAY}\")\n",
    "print(f\"\\nScheduler: ReduceLROnPlateau\")\n",
    "print(f\"  Factor: 0.5\")\n",
    "print(f\"  Patience: 3 epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8c1937",
   "metadata": {},
   "source": [
    "## 10. Main Training Loop\n",
    "\n",
    "Execute the full training loop with validation, checkpointing, and metric tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ec965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'test_loss': [],\n",
    "    'train_img2txt_acc': [],\n",
    "    'train_txt2img_acc': [],\n",
    "    'val_img2txt_acc': [],\n",
    "    'val_txt2img_acc': [],\n",
    "    'test_img2txt_acc': [],\n",
    "    'test_txt2img_acc': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0.0\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting Training for {config.NUM_EPOCHS} epochs\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for epoch in range(1, config.NUM_EPOCHS + 1):\n",
    "    print(f\"\\n--- Epoch {epoch}/{config.NUM_EPOCHS} ---\")\n",
    "    \n",
    "    # Train\n",
    "    train_metrics = train_epoch(model, train_loader, optimizer, \n",
    "                                triplet_loss_fn, device, epoch)\n",
    "    \n",
    "    # Validate\n",
    "    val_metrics = validate(model, val_loader, triplet_loss_fn, \n",
    "                          device, epoch, split_name=\"Val\")\n",
    "    \n",
    "    # Test\n",
    "    test_metrics = validate(model, test_loader, triplet_loss_fn, \n",
    "                           device, epoch, split_name=\"Test\")\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(val_metrics['loss'])\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Store metrics\n",
    "    history['train_loss'].append(train_metrics['loss'])\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['test_loss'].append(test_metrics['loss'])\n",
    "    history['val_img2txt_acc'].append(val_metrics['img2txt_acc'])\n",
    "    history['val_txt2img_acc'].append(val_metrics['txt2img_acc'])\n",
    "    history['test_img2txt_acc'].append(test_metrics['img2txt_acc'])\n",
    "    history['test_txt2img_acc'].append(test_metrics['txt2img_acc'])\n",
    "    history['learning_rates'].append(current_lr)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch} Summary:\")\n",
    "    print(f\"  Train Loss: {train_metrics['loss']:.4f}\")\n",
    "    print(f\"  Val Loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"  Test Loss: {test_metrics['loss']:.4f}\")\n",
    "    print(f\"  Val Image->Text Acc: {val_metrics['img2txt_acc']:.4f}\")\n",
    "    print(f\"  Val Text->Image Acc: {val_metrics['txt2img_acc']:.4f}\")\n",
    "    print(f\"  Test Image->Text Acc: {test_metrics['img2txt_acc']:.4f}\")\n",
    "    print(f\"  Test Text->Image Acc: {test_metrics['txt2img_acc']:.4f}\")\n",
    "    print(f\"  Learning Rate: {current_lr:.2e}\")\n",
    "    \n",
    "    # Save best model based on validation loss\n",
    "    if val_metrics['loss'] < best_val_loss:\n",
    "        best_val_loss = val_metrics['loss']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_acc': val_metrics['avg_acc'],\n",
    "            'config': {\n",
    "                'embedding_dim': config.EMBEDDING_DIM,\n",
    "                'margin': config.MARGIN,\n",
    "                'vocab_size': config.VOCAB_SIZE,\n",
    "                'text_embed_dim': config.TEXT_EMBED_DIM,\n",
    "                'text_hidden_dim': config.TEXT_HIDDEN_DIM\n",
    "            }\n",
    "        }, os.path.join(config.OUTPUT_DIR, 'best_model.pth'))\n",
    "        print(f\"  ✓ Saved best model (loss: {val_metrics['loss']:.4f})\")\n",
    "    \n",
    "    # Save best model based on validation accuracy\n",
    "    if val_metrics['avg_acc'] > best_val_acc:\n",
    "        best_val_acc = val_metrics['avg_acc']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_acc': val_metrics['avg_acc'],\n",
    "            'config': {\n",
    "                'embedding_dim': config.EMBEDDING_DIM,\n",
    "                'margin': config.MARGIN,\n",
    "                'vocab_size': config.VOCAB_SIZE,\n",
    "                'text_embed_dim': config.TEXT_EMBED_DIM,\n",
    "                'text_hidden_dim': config.TEXT_HIDDEN_DIM\n",
    "            }\n",
    "        }, os.path.join(config.OUTPUT_DIR, 'best_model_acc.pth'))\n",
    "        print(f\"  ✓ Saved best accuracy model (acc: {val_metrics['avg_acc']:.4f})\")\n",
    "    \n",
    "    # Save checkpoint every N epochs\n",
    "    if epoch % config.SAVE_FREQ == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_acc': val_metrics['avg_acc']\n",
    "        }, os.path.join(config.OUTPUT_DIR, f'checkpoint_epoch_{epoch}.pth'))\n",
    "        print(f\"  ✓ Saved checkpoint at epoch {epoch}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc83e6ae",
   "metadata": {},
   "source": [
    "## 11. Save Training History\n",
    "\n",
    "Save training history and model metadata for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a66a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history_path = os.path.join(config.OUTPUT_DIR, 'training_history.json')\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"\\nTraining history saved to: {history_path}\")\n",
    "\n",
    "# Save vocabulary\n",
    "vocab_path = os.path.join(config.OUTPUT_DIR, 'vocabulary.json')\n",
    "with open(vocab_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'word2idx': vocab.word2idx,\n",
    "        'idx2word': vocab.idx2word\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Vocabulary saved to: {vocab_path}\")\n",
    "\n",
    "# Save class mappings\n",
    "class_mapping_path = os.path.join(config.OUTPUT_DIR, 'class_mapping.json')\n",
    "with open(class_mapping_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'class_to_id': class_to_id,\n",
    "        'class_names': class_names\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Class mapping saved to: {class_mapping_path}\")\n",
    "\n",
    "# Save configuration\n",
    "config_path = os.path.join(config.OUTPUT_DIR, 'training_config.json')\n",
    "config_dict = {\n",
    "    'embedding_dim': config.EMBEDDING_DIM,\n",
    "    'image_size': config.IMAGE_SIZE,\n",
    "    'text_max_length': config.TEXT_MAX_LENGTH,\n",
    "    'vocab_size': config.VOCAB_SIZE,\n",
    "    'text_embed_dim': config.TEXT_EMBED_DIM,\n",
    "    'text_hidden_dim': config.TEXT_HIDDEN_DIM,\n",
    "    'batch_size': config.BATCH_SIZE,\n",
    "    'num_epochs': config.NUM_EPOCHS,\n",
    "    'learning_rate': config.LEARNING_RATE,\n",
    "    'weight_decay': config.WEIGHT_DECAY,\n",
    "    'margin': config.MARGIN,\n",
    "    'mining_strategy': config.MINING_STRATEGY,\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'best_val_acc': best_val_acc\n",
    "}\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "\n",
    "print(f\"Training configuration saved to: {config_path}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All training artifacts saved successfully!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abdaccd",
   "metadata": {},
   "source": [
    "## 12. Visualize Training Progress\n",
    "\n",
    "Plot training and testing metrics to analyze model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc8b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# 1. Training and Validation Loss\n",
    "axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].plot(epochs, history['test_loss'], 'g-', label='Test Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training, Validation, and Test Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Image-to-Text Retrieval Accuracy\n",
    "axes[0, 1].plot(epochs, history['val_img2txt_acc'], 'r-', label='Val Image->Text', linewidth=2)\n",
    "axes[0, 1].plot(epochs, history['test_img2txt_acc'], 'g-', label='Test Image->Text', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0, 1].set_title('Image-to-Text Retrieval Accuracy (Top-5)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_ylim([0, 1])\n",
    "\n",
    "# 3. Text-to-Image Retrieval Accuracy\n",
    "axes[1, 0].plot(epochs, history['val_txt2img_acc'], 'r-', label='Val Text->Image', linewidth=2)\n",
    "axes[1, 0].plot(epochs, history['test_txt2img_acc'], 'g-', label='Test Text->Image', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1, 0].set_title('Text-to-Image Retrieval Accuracy (Top-5)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "\n",
    "# 4. Average Retrieval Accuracy (Val + Test)\n",
    "val_avg_acc = [(i2t + t2i) / 2 for i2t, t2i in zip(history['val_img2txt_acc'], history['val_txt2img_acc'])]\n",
    "test_avg_acc = [(i2t + t2i) / 2 for i2t, t2i in zip(history['test_img2txt_acc'], history['test_txt2img_acc'])]\n",
    "\n",
    "axes[1, 1].plot(epochs, val_avg_acc, 'r-', label='Validation Avg', linewidth=2)\n",
    "axes[1, 1].plot(epochs, test_avg_acc, 'g-', label='Test Avg', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1, 1].set_title('Average Retrieval Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.OUTPUT_DIR, 'training_metrics.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining visualization saved to: {os.path.join(config.OUTPUT_DIR, 'training_metrics.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ec265",
   "metadata": {},
   "source": [
    "## 13. Additional Visualizations\n",
    "\n",
    "Detailed visualizations for loss components and learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cde553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Schedule\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "ax.plot(epochs, history['learning_rates'], 'b-', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Learning Rate', fontsize=12)\n",
    "ax.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.OUTPUT_DIR, 'learning_rate_schedule.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Learning rate schedule saved to: {os.path.join(config.OUTPUT_DIR, 'learning_rate_schedule.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed9fac7",
   "metadata": {},
   "source": [
    "## 14. Final Summary and Model Information\n",
    "\n",
    "Display final training statistics and saved model information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*25 + \"TRAINING COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nDataset Information:\")\n",
    "print(f\"  Total samples: {len(full_dataset):,}\")\n",
    "print(f\"  Number of classes: {len(class_names)}\")\n",
    "print(f\"  Train samples: {len(train_data):,}\")\n",
    "print(f\"  Validation samples: {len(val_data):,}\")\n",
    "print(f\"  Test samples: {len(test_data):,}\")\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(f\"  Embedding dimension: {config.EMBEDDING_DIM}\")\n",
    "print(f\"  Image encoder: ResNet-50 + Projection Head\")\n",
    "print(f\"  Text encoder: Embedding + BiLSTM + Projection Head\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(f\"  Epochs: {config.NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"  Initial learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"  Final learning rate: {history['learning_rates'][-1]:.2e}\")\n",
    "print(f\"  Triplet margin: {config.MARGIN}\")\n",
    "print(f\"  Mining strategy: {config.MINING_STRATEGY}\")\n",
    "\n",
    "print(\"\\nBest Performance:\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "print(\"\\nFinal Test Performance:\")\n",
    "print(f\"  Test loss: {history['test_loss'][-1]:.4f}\")\n",
    "print(f\"  Test Image->Text accuracy: {history['test_img2txt_acc'][-1]:.4f}\")\n",
    "print(f\"  Test Text->Image accuracy: {history['test_txt2img_acc'][-1]:.4f}\")\n",
    "print(f\"  Test average accuracy: {test_avg_acc[-1]:.4f}\")\n",
    "\n",
    "print(\"\\nSaved Files:\")\n",
    "print(f\"  Model (best loss): {os.path.join(config.OUTPUT_DIR, 'best_model.pth')}\")\n",
    "print(f\"  Model (best accuracy): {os.path.join(config.OUTPUT_DIR, 'best_model_acc.pth')}\")\n",
    "print(f\"  Training history: {os.path.join(config.OUTPUT_DIR, 'training_history.json')}\")\n",
    "print(f\"  Vocabulary: {os.path.join(config.OUTPUT_DIR, 'vocabulary.json')}\")\n",
    "print(f\"  Class mapping: {os.path.join(config.OUTPUT_DIR, 'class_mapping.json')}\")\n",
    "print(f\"  Configuration: {os.path.join(config.OUTPUT_DIR, 'training_config.json')}\")\n",
    "print(f\"  Visualizations: {os.path.join(config.OUTPUT_DIR, '*.png')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Next Steps:\")\n",
    "print(\"  1. Review training visualizations to assess model convergence\")\n",
    "print(\"  2. Proceed to inference notebook for model evaluation\")\n",
    "print(\"  3. Build vector database for similarity search\")\n",
    "print(\"  4. Integrate with recommendation system\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
